Ray Serve and Ray LLM
https://www.anyscale.com/blog/building-production-ai-applications-with-ray-serve

Enabling Cost-Efficient LLM Serving with Ray Serve
https://www.youtube.com/watch?v=TJ5K1CO9Wbs

Deploying Many Models Efficiently with Ray Serve
https://www.youtube.com/watch?v=QUYucglQzBw

Azure AI Toolchain Operator (Kaito)
https://github.com/Azure/kaito/tree/main

https://ray-project.github.io/kuberay/deploy/helm/
https://www.anyscale.com/blog/ray-serve-fastapi-the-best-of-both-worlds
https://www.anyscale.com/blog/simplify-your-mlops-with-ray-and-ray-serve
https://www.anyscale.com/blog/multi-model-composition-with-ray-serve-deployment-graphs

https://linuxhint.com/get-aws-session-token/

KubeRay: A Ray cluster management solution on Kubernetes

https://www.youtube.com/watch?v=1vGb0nn5n0o

https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.html

Ray is a powerful distributed computing system that can dynamically scale to meet changing demands, and with Azure's Spot Instances, it can do so at a huge discount.

https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-terraform?tabs=bash
https://xaviergeerinck.com/2023/05/02/deploying-ray-on-kubernetes-with-azure-spot-instances/
https://learnk8s.io/terraform-aks

NCasT4v3
kubectl apply -f serve_configs/meta-llama--Llama-2-7b-chat-hf.yaml

Code: PreconditionFailed
Message: Provisioning of resource(s) for Agent Pool genairaypl failed. Error: {
  "code": "InvalidTemplateDeployment",
  "message": "The template deployment 'bdb00e9c-a5a3-45a0-8612-7f64590adc68' is not valid according to the validation procedure. The tracking id is 'f744e3fa-81da-45fd-b9af-ecc5386f29be'. See inner errors for details.",
  "details": [
   {
    "code": "QuotaExceeded",
    "message": "Operation could not be completed as it results in exceeding approved LowPriorityCores quota. Additional details - Deployment Model: Resource Manager, Location: eastus, Current Limit: 3, Current Usage: 0, Additional Required: 4, (Minimum) New Limit Required: 4. Submit a request for Quota increase at https://aka.ms/ProdportalCRP/#blade/Microsoft_Azure_Capacity/UsageAndQuota.ReactView/Parameters/%7B%22subscriptionId%22:%22986ed5b7-8b65-4bae-87c4-64d7da3e019c%22,%22command%22:%22openQuotaApprovalBlade%22,%22quotas%22:[%7B%22location%22:%22eastus%22,%22providerId%22:%22Microsoft.Compute%22,%22resourceName%22:%22lowPriorityCores%22,%22quotaRequest%22:%7B%22properties%22:%7B%22limit%22:4,%22unit%22:%22Count%22,%22name%22:%7B%22value%22:%22lowPriorityCores%22%7D%7D%7D%7D]%7D by specifying parameters listed in the ‘Details’ section for deployment to succeed. Please read more about quota limits at https://docs.microsoft.com/en-us/azure/azure-portal/supportability/low-priority-quota"
   }

https://github.com/NVIDIA/k8s-device-plugin?ref=blog.truefoundry.com
https://learn.microsoft.com/en-us/azure/aks/gpu-cluster

az aks create -g GenerativeAI -n GenaiRayClst --enable-managed-identity --node-count 1 --enable-addons monitoring --enable-msi-auth-for-monitoring  --generate-ssh-keys --location southindia
az aks get-credentials --resource-group GenerativeAI --name GenaiRayClst
kubectl get nodes

az config set defaults.location=southindia

export HF_TOKEN=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
kubectl create secret generic hf-secret --from-literal=hf_api_token=${HF_TOKEN} --dry-run=client -o yaml > hf-secret.yaml

each of those tasks/actors can have fine grained resources allocation allowing for more efficiency utilization on each host without sacrificing latency (i.e., ray_actor_options={"num_cpus": 0.5}

Ray Serve allows you to configure the number of replicas at each step of your pipeline. This allows you to autoscale your ML serving application in milliseconds granularly.


Microbatching requests can also be implemented by using the @serve.batch decorator. This not only gives the developer a reusable abstraction, but also more flexibility and customization opportunity as part of their batching logic.



Zero copy load allows for loading large models in milliseconds or 340x times faster using Ray. Model caching will allow you to keep a pool of models in the Ray internal memory and allow you to hotswap with models for a given endpoint. This allows you to have many more models available than what your host can handle and allows you to optimize resources on your endpoint host depending on traffic, demand, or business rules.


az aks nodepool add \
   --resource-group GenerativeAI \
   --cluster-name GenaiRayClst \
   --name genairaypl \
   --priority Spot \
   --eviction-policy Delete \
   --node-vm-size Standard_NC8as_T4_v3 \
   --node-count 2 \
   --node-taints kubernetes.azure.com/scalesetpriority=spot:NoSchedule \
   --labels agentpool=genairaypl \
   --mode User \
   --no-wait

   az aks nodepool add \
   --resource-group GenerativeAI \
   --cluster-name GenaiRayClst \
   --name genairaypl \
   --eviction-policy Delete \
   --node-vm-size Standard_NC8as_T4_v3 \
   --node-count 2 \
   --labels agentpool=genairaypl \
   --mode User \
   --no-wait

watch --color --interval 5 --no-title "kubectl exec -n default -it rayllm-raycluster-49ldz-head-bl5cg -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'"


az aks nodepool add -g MyResourceGroup -n nodepool1 --cluster-name MyManagedCluster --node-osdisk-type Ephemeral --node-osdisk-size 48
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
# Install Ray locally
pip install -U ray[default]

# Add the helm chart
helm repo add kuberay https://ray-project.github.io/kuberay-helm
helm repo update


# Install Kuberay
helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm install kuberay-operator kuberay/kuberay-operator \
    --version 0.5.0 \
    --set "nodeSelector.agentpool"="genairaypl" \
    --set "tolerations[0].key"="kubernetes.azure.com/scalesetpriority" \
    --set "tolerations[0].operator"="Equal" \
    --set "tolerations[0].value"="spot" \
    --set "tolerations[0].effect"="NoSchedule"

 g5.xlarge
  m5.xlarge
helm install kuberay-operator kuberay/kuberay-operator --version 0.5.0 --set "nodeSelector.agentpool"="genairaypl"
helm install kuberay-operator kuberay/kuberay-operator --version 1.0.0
helm install raycluster kuberay/ray-cluster --version 1.0.0

# Deploy RayCluster
helm install raycluster kuberay/ray-cluster \
    --version 0.5.0 \
    --set "nodeSelector.agentpool"="ray" \
    --set "tolerations[0].key"="kubernetes.azure.com/scalesetpriority" \
    --set "tolerations[0].operator"="Equal" \
    --set "tolerations[0].value"="spot" \
    --set "tolerations[0].effect"="NoSchedule"

# Deploy RayCluster
helm install raycluster kuberay/ray-cluster --version 0.5.0 --set "nodeSelector.agentpool"="genairaypl"
helm install raycluster kuberay/ray-cluster --version 1.0.0
# Await all pods to be running
while [ $(kubectl get pods --selector=ray.io/cluster=raycluster-kuberay | grep Running | wc -l) -lt 2 ]; do echo "Awaiting Running Pods..."; sleep 1; done

# Access the cluster
export HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)

ServeReplica:ray-llm:VLLMDeployment:meta-llama--Llama-2-7b-chat-hf  PENDING_CREATION

kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265

export HF_TOKEN=HUGGING_FACE_TOKEN
kubectl create secret generic hf-secret \
    --from-literal=hf_api_token=${HF_TOKEN} \
    --dry-run=client -o yaml > hf-secret.yaml

kubectl apply -f hf-secret.yaml
kubectl apply -f models/llama2-7b-chat-hf.yaml
kubectl apply -f llama2-7b.yaml


kubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log

kubectl exec -it service/rayllm-raycluster-shvtn-head-svc -- ray summary actors
kubectl exec -it rayllm-raycluster-49ldz-head-bl5cg -- bash

watch --color --interval 5 --no-title "kubectl exec -n default -it rayllm-raycluster-ksp7z-head-7q94k -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'"

export HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)
kubectl exec -it rayllm-raycluster-ksp7z-head-7q94k -- ray summary actors

https://docs.ray.io/en/master/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html#kuberay-raysvc-troubleshoot

kubectl port-forward service/rayllm-raycluster-shvtn-head-svc 8000:8000

ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5.

curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What are the top 5 most popular programming languages? Please be brief."}
    ],
    "temperature": 0.7
  }'